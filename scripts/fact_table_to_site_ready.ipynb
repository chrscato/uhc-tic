{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d975b944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (658968, 54)\n",
      "Columns: ['rate_uuid', 'payer_uuid', 'organization_uuid', 'service_code', 'service_description', 'billing_code_type', 'negotiated_rate', 'billing_class', 'rate_type', 'service_codes', 'plan_details', 'contract_period', 'provider_network', 'geographic_scope', 'data_lineage', 'created_at', 'updated_at', 'quality_flags', 'rate_npis', 'tin', 'organization_name', 'organization_type', 'parent_system', 'npi_count', 'primary_specialty', 'is_facility', 'headquarters_address', 'service_areas', 'created_at_org', 'updated_at_org', 'data_quality_score', 'npi', 'nppes_provider_type', 'nppes_primary_specialty', 'nppes_gender', 'nppes_addresses', 'nppes_credentials', 'nppes_provider_name', 'nppes_enumeration_date', 'nppes_last_updated', 'nppes_secondary_specialties', 'nppes_metadata', 'nppes_city', 'nppes_state', 'nppes_zip', 'nppes_country', 'nppes_street', 'nppes_phone', 'nppes_fax', 'nppes_address_type', 'nppes_address_purpose', 'rate_category', 'service_category', 'fact_key']\n",
      "Memory usage: 2739.8 MB\n",
      "Successfully loaded from s3://commercial-rates/tic-mrf/consolidated/centene_fidelis/fact_tables/memory_efficient_fact_table_20250802_183825.parquet\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "import io\n",
    "from pathlib import Path\n",
    "\n",
    "# S3 configuration\n",
    "s3_bucket = \"commercial-rates\"  # or use os.getenv('S3_BUCKET')\n",
    "s3_prefix = \"tic-mrf\"\n",
    "specific_fact_table = \"/consolidated/centene_fidelis/fact_tables/memory_efficient_fact_table_20250802_183825.parquet\"      # or use os.getenv('S3_PREFIX')\n",
    "s3_key = f\"{s3_prefix}{specific_fact_table}\"  \n",
    "# Initialize S3 client\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "try:\n",
    "    # Get object from S3\n",
    "    response = s3_client.get_object(Bucket=s3_bucket, Key=s3_key)\n",
    "    \n",
    "    # Read parquet directly from memory buffer\n",
    "    df = pd.read_parquet(io.BytesIO(response['Body'].read()))\n",
    "    \n",
    "    # Basic info\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"Columns: {list(df.columns)}\")\n",
    "    print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024 / 1024:.1f} MB\")\n",
    "    print(f\"Successfully loaded from s3://{s3_bucket}/{s3_key}\")\n",
    "\n",
    "    # Show first few rows\n",
    "    df.head()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading from S3: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6df536e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import numpy as np\n",
    "\n",
    "def parse_nppes_addresses(addr_str):\n",
    "    \"\"\"Parse the nppes_addresses string into a list of dictionaries.\"\"\"\n",
    "    if pd.isna(addr_str) or addr_str == '':\n",
    "        return []\n",
    "    \n",
    "    # Remove the numpy array wrapper if present\n",
    "    addr_str = addr_str.strip()\n",
    "    if addr_str.startswith('array('):\n",
    "        # Extract the content between the outer brackets\n",
    "        addr_str = addr_str[addr_str.find('['):addr_str.rfind(']')+1]\n",
    "    \n",
    "    try:\n",
    "        # Now we can safely evaluate the list of dictionaries\n",
    "        return ast.literal_eval(addr_str)\n",
    "    except:\n",
    "        # Return empty list if parsing fails\n",
    "        return []\n",
    "\n",
    "# Parse the nppes_addresses column\n",
    "df['nppes_address_list'] = df['nppes_addresses'].apply(parse_nppes_addresses)\n",
    "\n",
    "# Verify the results\n",
    "print(\"Sample of parsed addresses:\")\n",
    "print(df['nppes_address_list'].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b890503d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract first address fields (if you need them)\n",
    "df['first_address'] = df['nppes_address_list'].apply(lambda x: x[0] if len(x) > 0 else {})\n",
    "\n",
    "# Or extract specific fields from first address\n",
    "df['address_street'] = df['nppes_address_list'].apply(lambda x: x[0].get('street', '') if len(x) > 0 else '')\n",
    "df['address_city'] = df['nppes_address_list'].apply(lambda x: x[0].get('city', '') if len(x) > 0 else '')\n",
    "df['address_state'] = df['nppes_address_list'].apply(lambda x: x[0].get('state', '') if len(x) > 0 else '')\n",
    "df['address_zip'] = df['nppes_address_list'].apply(lambda x: x[0].get('zip', '') if len(x) > 0 else '')\n",
    "\n",
    "# Show the extracted fields\n",
    "print(\"\\nSample of extracted address fields:\")\n",
    "print(df[['address_street', 'address_city', 'address_state', 'address_zip']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5123814",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "330758ce",
   "metadata": {},
   "source": [
    "--inspect s3 parquets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b1ee0f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 payer files:\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"Simple script to inspect payer parquet files in S3.\"\"\"\n",
    "\n",
    "import boto3\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import tempfile\n",
    "\n",
    "def inspect_payer_parquets(bucket=\"commercial-rates\", prefix=\"tic-mrf/test\"):\n",
    "    \"\"\"Inspect payer parquet files in S3.\"\"\"\n",
    "    s3_client = boto3.client('s3')\n",
    "    \n",
    "    # List payer files\n",
    "    paginator = s3_client.get_paginator('list_objects_v2')\n",
    "    pages = paginator.paginate(Bucket=bucket, Prefix=prefix)\n",
    "    \n",
    "    payer_files = []\n",
    "    for page in pages:\n",
    "        if 'Contents' in page:\n",
    "            for obj in page['Contents']:\n",
    "                if 'payers' in obj['Key'] and obj['Key'].endswith('.parquet'):\n",
    "                    payer_files.append(obj['Key'])\n",
    "    \n",
    "    print(f\"Found {len(payer_files)} payer files:\")\n",
    "    for i, file_key in enumerate(payer_files, 1):\n",
    "        print(f\"  {i}. {file_key}\")\n",
    "    \n",
    "    # Inspect first file in detail\n",
    "    if payer_files:\n",
    "        print(f\"\\nInspecting first file: {payer_files[0]}\")\n",
    "        \n",
    "        # Download and read\n",
    "        with tempfile.NamedTemporaryFile(suffix='.parquet') as tmp_file:\n",
    "            s3_client.download_file(bucket, payer_files[0], tmp_file.name)\n",
    "            df = pd.read_parquet(tmp_file.name)\n",
    "        \n",
    "        print(f\"Shape: {df.shape}\")\n",
    "        print(f\"Columns: {list(df.columns)}\")\n",
    "        print(f\"\\nSample data:\")\n",
    "        print(df.head(3).to_string())\n",
    "        \n",
    "        # Check payer_uuid mapping\n",
    "        if 'payer_uuid' in df.columns:\n",
    "            print(f\"\\nUnique payers: {df['payer_uuid'].nunique()}\")\n",
    "            print(f\"Sample UUIDs: {df['payer_uuid'].head().tolist()}\")\n",
    "        \n",
    "        # Check payer names\n",
    "        if 'payer_name' in df.columns:\n",
    "            print(f\"\\nPayer names: {df['payer_name'].unique()}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    inspect_payer_parquets()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289e0fa5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2003885e",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "f-string: expecting a valid expression after '{' (3457547917.py, line 160)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 160\u001b[1;36m\u001b[0m\n\u001b[1;33m    print(f\"  Difference: {len(original_df) - streaming_count:,} ({((len(original_df) - streaming_count)/len(original_df)*100:.1f}%)\")\u001b[0m\n\u001b[1;37m                                                                   ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m f-string: expecting a valid expression after '{'\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Review S3 streaming fact table results.\n",
    "Analyzes the streaming chunks and provides insights on the data.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import json\n",
    "from datetime import datetime\n",
    "import io\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def review_s3_streaming_results():\n",
    "    \"\"\"Review the S3 streaming fact table results.\"\"\"\n",
    "    \n",
    "    # S3 configuration\n",
    "    s3_bucket = \"commercial-rates\"\n",
    "    s3_prefix = \"tic-mrf/test\"\n",
    "    chunk_key = \"tic-mrf/test/fact_tables/streaming_chunks/chunk_0000_20250724_091055.parquet\"\n",
    "    summary_key = \"tic-mrf/test/s3_streaming_fact_table_summary.json\"\n",
    "    \n",
    "    # Initialize S3 client\n",
    "    try:\n",
    "        s3_client = boto3.client('s3')\n",
    "        logger.info(\"âœ… S3 client initialized successfully\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"âŒ Failed to initialize S3 client: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Load the streaming chunk\n",
    "    logger.info(f\"Loading streaming chunk from s3://{s3_bucket}/{chunk_key}\")\n",
    "    try:\n",
    "        response = s3_client.get_object(Bucket=s3_bucket, Key=chunk_key)\n",
    "        df = pd.read_parquet(io.BytesIO(response['Body'].read()))\n",
    "        logger.info(f\"âœ… Loaded {len(df):,} records from streaming chunk\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"âŒ Failed to load streaming chunk: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Basic statistics\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ðŸ“Š S3 STREAMING FACT TABLE ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"\\nðŸ“ˆ RECORD COUNT: {len(df):,} records\")\n",
    "    print(f\"ðŸ“ FILE SIZE: {response['ContentLength'] / 1024 / 1024:.1f} MB\")\n",
    "    \n",
    "    # Column analysis\n",
    "    print(f\"\\nðŸ“‹ COLUMNS ({len(df.columns)} total):\")\n",
    "    for i, col in enumerate(df.columns, 1):\n",
    "        print(f\"  {i:2d}. {col}\")\n",
    "    \n",
    "    # Sample data\n",
    "    print(f\"\\nðŸ” SAMPLE DATA (first 3 records):\")\n",
    "    print(df.head(3).to_string())\n",
    "    \n",
    "    # NPPES enrichment analysis\n",
    "    if 'npi' in df.columns:\n",
    "        npi_count = df['npi'].notna().sum()\n",
    "        print(f\"\\nðŸ¥ NPPES ENRICHMENT:\")\n",
    "        print(f\"  Records with NPI: {npi_count:,} ({npi_count/len(df)*100:.1f}%)\")\n",
    "        print(f\"  Records without NPI: {len(df) - npi_count:,} ({(len(df) - npi_count)/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    # Service code analysis\n",
    "    if 'service_code' in df.columns:\n",
    "        service_counts = df['service_code'].value_counts().head(10)\n",
    "        print(f\"\\nðŸ¥ TOP 10 SERVICE CODES:\")\n",
    "        for code, count in service_counts.items():\n",
    "            print(f\"  {code}: {count:,} records\")\n",
    "    \n",
    "    # Service category analysis\n",
    "    if 'service_category' in df.columns:\n",
    "        category_counts = df['service_category'].value_counts()\n",
    "        print(f\"\\nðŸ“Š SERVICE CATEGORIES:\")\n",
    "        for category, count in category_counts.items():\n",
    "            print(f\"  {category}: {count:,} records ({count/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    # Organization analysis\n",
    "    if 'organization_name' in df.columns:\n",
    "        org_counts = df['organization_name'].value_counts().head(5)\n",
    "        print(f\"\\nðŸ¢ TOP 5 ORGANIZATIONS:\")\n",
    "        for org, count in org_counts.items():\n",
    "            print(f\"  {org}: {count:,} records\")\n",
    "    \n",
    "    # Rate analysis\n",
    "    if 'negotiated_rate' in df.columns:\n",
    "        rates = df['negotiated_rate'].dropna()\n",
    "        if len(rates) > 0:\n",
    "            print(f\"\\nðŸ’° RATE ANALYSIS:\")\n",
    "            print(f\"  Min rate: ${rates.min():,.2f}\")\n",
    "            print(f\"  Max rate: ${rates.max():,.2f}\")\n",
    "            print(f\"  Mean rate: ${rates.mean():,.2f}\")\n",
    "            print(f\"  Median rate: ${rates.median():,.2f}\")\n",
    "    \n",
    "    # NPI analysis\n",
    "    if 'npi' in df.columns:\n",
    "        unique_npis = df['npi'].nunique()\n",
    "        print(f\"\\nðŸ‘¨â€âš•ï¸ PROVIDER ANALYSIS:\")\n",
    "        print(f\"  Unique NPIs: {unique_npis:,}\")\n",
    "        print(f\"  Average records per NPI: {len(df)/unique_npis:.1f}\")\n",
    "    \n",
    "    # Load and display summary\n",
    "    try:\n",
    "        summary_response = s3_client.get_object(Bucket=s3_bucket, Key=summary_key)\n",
    "        summary_data = json.loads(summary_response['Body'].read().decode('utf-8'))\n",
    "        print(f\"\\nðŸ“‹ S3 SUMMARY METADATA:\")\n",
    "        for key, value in summary_data.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Could not load summary: {e}\")\n",
    "    \n",
    "    # Data quality checks\n",
    "    print(f\"\\nðŸ” DATA QUALITY CHECKS:\")\n",
    "    \n",
    "    # Check for missing values\n",
    "    missing_counts = df.isnull().sum()\n",
    "    columns_with_missing = missing_counts[missing_counts > 0]\n",
    "    if len(columns_with_missing) > 0:\n",
    "        print(f\"  Columns with missing values:\")\n",
    "        for col, count in columns_with_missing.items():\n",
    "            print(f\"    {col}: {count:,} missing ({count/len(df)*100:.1f}%)\")\n",
    "    else:\n",
    "        print(\"  âœ… No missing values found\")\n",
    "    \n",
    "    # Check for duplicates\n",
    "    duplicates = df.duplicated().sum()\n",
    "    print(f\"  Duplicate records: {duplicates:,} ({duplicates/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"âœ… S3 STREAMING REVIEW COMPLETE\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "def compare_with_original():\n",
    "    \"\"\"Compare streaming results with original fact table.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ðŸ”„ COMPARISON WITH ORIGINAL FACT TABLE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Load original fact table\n",
    "    original_file = Path(\"dashboard_data/memory_efficient_fact_table.parquet\")\n",
    "    if original_file.exists():\n",
    "        original_df = pd.read_parquet(original_file)\n",
    "        print(f\"\\nðŸ“Š ORIGINAL FACT TABLE:\")\n",
    "        print(f\"  Records: {len(original_df):,}\")\n",
    "        print(f\"  File size: {original_file.stat().st_size / 1024 / 1024:.1f} MB\")\n",
    "        print(f\"  Columns: {len(original_df.columns)}\")\n",
    "        \n",
    "        # Compare record counts\n",
    "        streaming_count = 17067  # From the logs\n",
    "        print(f\"\\nðŸ“ˆ COMPARISON:\")\n",
    "        print(f\"  Original records: {len(original_df):,}\")\n",
    "        print(f\"  Streaming records: {streaming_count:,}\")\n",
    "        print(f\"  Difference: {len(original_df) - streaming_count:,} ({((len(original_df) - streaming_count)/len(original_df)*100:.1f}%)\")\n",
    "        \n",
    "        # Check if streaming is subset\n",
    "        if streaming_count < len(original_df):\n",
    "            print(f\"  âš ï¸  Streaming processed fewer records (test mode limited to 2 files)\")\n",
    "        else:\n",
    "            print(f\"  âœ… Streaming processed more records\")\n",
    "    else:\n",
    "        print(\"âŒ Original fact table not found\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"ðŸš€ Starting S3 streaming results review...\")\n",
    "    review_s3_streaming_results()\n",
    "    compare_with_original()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
